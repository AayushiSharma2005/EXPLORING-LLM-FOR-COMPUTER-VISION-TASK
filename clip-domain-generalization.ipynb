{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9545143,"sourceType":"datasetVersion","datasetId":5815134}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:37:55.734454Z","iopub.execute_input":"2025-07-17T14:37:55.735140Z","iopub.status.idle":"2025-07-17T14:39:06.597098Z","shell.execute_reply.started":"2025-07-17T14:37:55.735119Z","shell.execute_reply":"2025-07-17T14:39:06.596179Z"}},"outputs":[{"name":"stdout","text":"Collecting open_clip_torch\n  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.21.0+cu124)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2024.11.6)\nCollecting ftfy (from open_clip_torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.5.3)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (1.0.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open_clip_torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (1.1.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open_clip_torch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open_clip_torch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open_clip_torch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open_clip_torch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open_clip_torch) (2024.2.0)\nDownloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, open_clip_torch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open_clip_torch-2.32.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Images per domain# ","metadata":{}},{"cell_type":"code","source":"import os\n\nroot_dir = \"/kaggle/input/pacs-dataset/kfold\"\ndomains = [\"photo\", \"art_painting\", \"cartoon\", \"sketch\"]\n\nfor domain in domains:\n    domain_path = os.path.join(root_dir, domain)\n    count = 0\n    for cls in os.listdir(domain_path):\n        cls_path = os.path.join(domain_path, cls)\n        count += len(os.listdir(cls_path))\n    print(f\"{domain} has {count} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:39:23.363064Z","iopub.execute_input":"2025-07-17T14:39:23.363330Z","iopub.status.idle":"2025-07-17T14:39:23.675579Z","shell.execute_reply.started":"2025-07-17T14:39:23.363300Z","shell.execute_reply":"2025-07-17T14:39:23.674989Z"}},"outputs":[{"name":"stdout","text":"photo has 1670 images\nart_painting has 2048 images\ncartoon has 2344 images\nsketch has 3929 images\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Clip domain generalization using patience learning(test on cartoon)**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport open_clip\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\nclip_model.eval().to(device)\nfor param in clip_model.parameters():\n    param.requires_grad = False  # Freeze CLIP weights\n\n\nclass CLIPMLPClassifier(nn.Module):\n    def __init__(self, clip_model, num_classes):\n        super().__init__()\n        self.clip = clip_model\n        self.mlp = nn.Sequential(\n            nn.Linear(self.clip.visual.output_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        with torch.no_grad():\n            feats = self.clip.encode_image(x)\n        return self.mlp(feats)\n\n\ndef get_loader(domains, root, batch_size=32, shuffle=False):\n    dataset = []\n    for domain in domains:\n        ds = datasets.ImageFolder(os.path.join(root, domain), transform=preprocess)\n        dataset.extend(ds.samples)\n    base_ds = datasets.ImageFolder(os.path.join(root, domains[0]), transform=preprocess)\n    base_ds.samples = dataset\n    loader = DataLoader(base_ds, batch_size=batch_size, shuffle=shuffle)\n    return loader, len(base_ds.classes)\n\nPACS_PATH = \"/kaggle/input/pacs-dataset/kfold\"\ntrain_domains = [\"photo\", \"art_painting\", \"sketch\"]\ntest_domain = \"cartoon\"\n\ntrain_loader, num_classes = get_loader(train_domains, root=PACS_PATH, shuffle=True)\ntest_loader, _ = get_loader([test_domain], root=PACS_PATH, shuffle=False)\n\n\nmodel = CLIPMLPClassifier(clip_model, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.mlp.parameters(), lr=1e-4)\n\n# Early stopping \nbest_acc = 0\nwait = 0\npatience = 10\nbest_model_state = None\n\n\nfor epoch in range(50): \n    model.train()\n    total_loss = 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1} - Training Loss: {total_loss:.4f}\")\n\n   \n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = torch.argmax(outputs, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    acc = 100 * correct / total\n    print(f\"Test Accuracy on '{test_domain}': {acc:.2f}%\")\n\n    # Early stopping check\n    if acc > best_acc:\n        best_acc = acc\n        best_model_state = model.state_dict()\n        wait = 0\n        print(\"Accuracy improved. Model saved.\")\n    else:\n        wait += 1\n        print(f\"No improvement. Wait count: {wait}/{patience}\")\n        if wait >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n\nif best_model_state:\n    model.load_state_dict(best_model_state)\n    print(f\"Best model with {best_acc:.2f}% accuracy loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T15:28:15.604731Z","iopub.execute_input":"2025-07-17T15:28:15.605296Z","iopub.status.idle":"2025-07-17T15:40:08.057478Z","shell.execute_reply.started":"2025-07-17T15:28:15.605268Z","shell.execute_reply":"2025-07-17T15:40:08.056798Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 239/239 [00:39<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training Loss: 211.7789\nTest Accuracy on 'cartoon': 98.51%\nAccuracy improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 239/239 [00:39<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training Loss: 43.3481\nTest Accuracy on 'cartoon': 98.46%\nNo improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 239/239 [00:38<00:00,  6.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training Loss: 27.7725\nTest Accuracy on 'cartoon': 98.63%\nAccuracy improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 239/239 [00:38<00:00,  6.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training Loss: 23.0163\nTest Accuracy on 'cartoon': 98.72%\nAccuracy improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 239/239 [00:38<00:00,  6.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training Loss: 20.6862\nTest Accuracy on 'cartoon': 98.59%\nNo improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 239/239 [00:38<00:00,  6.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 - Training Loss: 19.0510\nTest Accuracy on 'cartoon': 98.34%\nNo improvement. Wait count: 2/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 239/239 [00:38<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 - Training Loss: 17.7896\nTest Accuracy on 'cartoon': 98.38%\nNo improvement. Wait count: 3/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 239/239 [00:39<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 - Training Loss: 16.9529\nTest Accuracy on 'cartoon': 98.25%\nNo improvement. Wait count: 4/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 239/239 [00:39<00:00,  6.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 - Training Loss: 15.9079\nTest Accuracy on 'cartoon': 98.21%\nNo improvement. Wait count: 5/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 239/239 [00:39<00:00,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 - Training Loss: 15.4416\nTest Accuracy on 'cartoon': 98.04%\nNo improvement. Wait count: 6/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 - Training: 100%|██████████| 239/239 [00:38<00:00,  6.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 - Training Loss: 14.7326\nTest Accuracy on 'cartoon': 98.29%\nNo improvement. Wait count: 7/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 - Training: 100%|██████████| 239/239 [00:38<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 - Training Loss: 14.3917\nTest Accuracy on 'cartoon': 98.42%\nNo improvement. Wait count: 8/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 - Training: 100%|██████████| 239/239 [00:39<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 - Training Loss: 13.7378\nTest Accuracy on 'cartoon': 98.12%\nNo improvement. Wait count: 9/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 - Training: 100%|██████████| 239/239 [00:39<00:00,  6.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 - Training Loss: 13.1868\nTest Accuracy on 'cartoon': 98.12%\nNo improvement. Wait count: 10/10\nEarly stopping triggered.\nBest model with 98.72% accuracy loaded.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Clip domain generalization using patience learning(test on art_painting)**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport open_clip\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\nclip_model.eval().to(device)\nfor param in clip_model.parameters():\n    param.requires_grad = False \n\n\nclass CLIPMLPClassifier(nn.Module):\n    def __init__(self, clip_model, num_classes):\n        super().__init__()\n        self.clip = clip_model\n        self.mlp = nn.Sequential(\n            nn.Linear(self.clip.visual.output_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        with torch.no_grad():\n            feats = self.clip.encode_image(x)\n        return self.mlp(feats)\n\n\ndef get_loader(domains, root, batch_size=32, shuffle=False):\n    dataset = []\n    for domain in domains:\n        ds = datasets.ImageFolder(os.path.join(root, domain), transform=preprocess)\n        dataset.extend(ds.samples)\n    base_ds = datasets.ImageFolder(os.path.join(root, domains[0]), transform=preprocess)\n    base_ds.samples = dataset\n    loader = DataLoader(base_ds, batch_size=batch_size, shuffle=shuffle)\n    return loader, len(base_ds.classes)\n\nPACS_PATH = \"/kaggle/input/pacs-dataset/kfold\"\ntrain_domains = [\"photo\", \"cartoon\", \"sketch\"]\ntest_domain = \"art_painting\"\n\ntrain_loader, num_classes = get_loader(train_domains, root=PACS_PATH, shuffle=True)\ntest_loader, _ = get_loader([test_domain], root=PACS_PATH, shuffle=False)\n\n\nmodel = CLIPMLPClassifier(clip_model, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.mlp.parameters(), lr=1e-4)\n\n# Early stopping \nbest_acc = 0\nwait = 0\npatience = 10\nbest_model_state = None\n\n\nfor epoch in range(50): \n    model.train()\n    total_loss = 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1} - Training Loss: {total_loss:.4f}\")\n\n   \n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = torch.argmax(outputs, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    acc = 100 * correct / total\n    print(f\"Test Accuracy on '{test_domain}': {acc:.2f}%\")\n\n    # Early stopping check\n    if acc > best_acc:\n        best_acc = acc\n        best_model_state = model.state_dict()\n        wait = 0\n        print(\"Accuracy improved. Model saved.\")\n    else:\n        wait += 1\n        print(f\"No improvement. Wait count: {wait}/{patience}\")\n        if wait >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n\nif best_model_state:\n    model.load_state_dict(best_model_state)\n    print(f\"Best model with {best_acc:.2f}% accuracy loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:02:58.809961Z","iopub.execute_input":"2025-07-17T16:02:58.810636Z","iopub.status.idle":"2025-07-17T16:14:53.880172Z","shell.execute_reply.started":"2025-07-17T16:02:58.810613Z","shell.execute_reply":"2025-07-17T16:14:53.879341Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training Loss: 201.9480\nTest Accuracy on 'art_painting': 97.31%\nAccuracy improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training Loss: 38.7605\nTest Accuracy on 'art_painting': 97.51%\nAccuracy improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training Loss: 25.7157\nTest Accuracy on 'art_painting': 97.51%\nNo improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training Loss: 21.6532\nTest Accuracy on 'art_painting': 97.71%\nAccuracy improved. Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 249/249 [00:41<00:00,  6.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training Loss: 19.4363\nTest Accuracy on 'art_painting': 97.71%\nNo improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 - Training Loss: 18.0842\nTest Accuracy on 'art_painting': 97.61%\nNo improvement. Wait count: 2/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 249/249 [00:41<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 - Training Loss: 16.9262\nTest Accuracy on 'art_painting': 97.36%\nNo improvement. Wait count: 3/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 - Training Loss: 16.1238\nTest Accuracy on 'art_painting': 97.41%\nNo improvement. Wait count: 4/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 - Training Loss: 15.3102\nTest Accuracy on 'art_painting': 97.36%\nNo improvement. Wait count: 5/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 - Training Loss: 14.5925\nTest Accuracy on 'art_painting': 97.31%\nNo improvement. Wait count: 6/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 - Training Loss: 14.1197\nTest Accuracy on 'art_painting': 97.12%\nNo improvement. Wait count: 7/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 - Training Loss: 13.5028\nTest Accuracy on 'art_painting': 97.07%\nNo improvement. Wait count: 8/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 - Training: 100%|██████████| 249/249 [00:41<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 - Training Loss: 12.9832\nTest Accuracy on 'art_painting': 96.92%\nNo improvement. Wait count: 9/10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 - Training: 100%|██████████| 249/249 [00:40<00:00,  6.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 - Training Loss: 12.8201\nTest Accuracy on 'art_painting': 96.83%\nNo improvement. Wait count: 10/10\nEarly stopping triggered.\nBest model with 97.71% accuracy loaded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**Domain Generalization using CLIP(test on cartoon**)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport open_clip\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\nclip_model.eval().to(device)\nfor param in clip_model.parameters():\n    param.requires_grad = False  # freeze CLIP\n\n#  classifier  (512 → 256 → 7)\nclass CLIPMLPClassifier(nn.Module):\n    def __init__(self, clip_model, num_classes):\n        super().__init__()\n        self.clip = clip_model\n        self.mlp = nn.Sequential(\n            nn.Linear(self.clip.visual.output_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        with torch.no_grad():\n            feats = self.clip.encode_image(x)\n        return self.mlp(feats)\n\n\ndef get_loader(domains, root, batch_size=32, shuffle=False):\n    dataset = []\n    for domain in domains:\n        ds = datasets.ImageFolder(os.path.join(root, domain), transform=preprocess)\n        dataset.extend(ds.samples)\n    base_ds = datasets.ImageFolder(os.path.join(root, domains[0]), transform=preprocess)\n    base_ds.samples = dataset\n    loader = DataLoader(base_ds, batch_size=batch_size, shuffle=shuffle)\n    return loader, len(base_ds.classes)\n\n\nPACS_PATH = \"/kaggle/input/pacs-dataset/kfold\"\ntrain_domains = [\"photo\", \"art_painting\", \"sketch\"]\ntest_domain = \"cartoon\"\n\ntrain_loader, num_classes = get_loader(train_domains, root=PACS_PATH, shuffle=True)\ntest_loader, _ = get_loader([test_domain], root=PACS_PATH, shuffle=False)\n\n\nmodel = CLIPMLPClassifier(clip_model, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.mlp.parameters(), lr=1e-4)\n\n# Train + evaluate\nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"\\nEpoch {epoch+1} Training Loss: {total_loss:.4f}\")\n\n    \n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = torch.argmax(outputs, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    acc = 100 * correct / total\n    print(f\"Test Accuracy on '{test_domain}': {acc:.2f}%\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:51:11.912034Z","iopub.execute_input":"2025-07-17T14:51:11.912601Z","iopub.status.idle":"2025-07-17T15:08:04.208763Z","shell.execute_reply.started":"2025-07-17T14:51:11.912577Z","shell.execute_reply":"2025-07-17T15:08:04.208158Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Training Loss: 212.3339\nTest Accuracy on 'cartoon': 98.46%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 [Training]: 100%|██████████| 239/239 [00:39<00:00,  5.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Training Loss: 42.9130\nTest Accuracy on 'cartoon': 98.63%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Training Loss: 27.4814\nTest Accuracy on 'cartoon': 98.51%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 [Training]: 100%|██████████| 239/239 [00:38<00:00,  6.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 Training Loss: 23.0446\nTest Accuracy on 'cartoon': 98.63%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 Training Loss: 20.5486\nTest Accuracy on 'cartoon': 98.34%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 [Training]: 100%|██████████| 239/239 [00:40<00:00,  5.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 Training Loss: 18.9613\nTest Accuracy on 'cartoon': 98.21%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 Training Loss: 17.7758\nTest Accuracy on 'cartoon': 98.81%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 Training Loss: 16.7435\nTest Accuracy on 'cartoon': 98.72%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9 Training Loss: 16.0775\nTest Accuracy on 'cartoon': 98.29%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 [Training]: 100%|██████████| 239/239 [00:38<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 Training Loss: 15.2508\nTest Accuracy on 'cartoon': 98.38%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11 Training Loss: 14.6685\nTest Accuracy on 'cartoon': 98.29%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 [Training]: 100%|██████████| 239/239 [00:38<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12 Training Loss: 13.9675\nTest Accuracy on 'cartoon': 98.46%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13 Training Loss: 13.6185\nTest Accuracy on 'cartoon': 98.21%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14 Training Loss: 13.1438\nTest Accuracy on 'cartoon': 98.46%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15 Training Loss: 12.6410\nTest Accuracy on 'cartoon': 98.08%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16 [Training]: 100%|██████████| 239/239 [00:38<00:00,  6.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16 Training Loss: 12.2375\nTest Accuracy on 'cartoon': 97.99%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17 [Training]: 100%|██████████| 239/239 [00:38<00:00,  6.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17 Training Loss: 11.8948\nTest Accuracy on 'cartoon': 98.17%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18 [Training]: 100%|██████████| 239/239 [00:38<00:00,  6.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18 Training Loss: 11.6934\nTest Accuracy on 'cartoon': 97.95%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19 Training Loss: 11.0945\nTest Accuracy on 'cartoon': 98.12%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20 [Training]: 100%|██████████| 239/239 [00:39<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20 Training Loss: 10.7250\nTest Accuracy on 'cartoon': 98.51%\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**Domain Generalization using CLIP(on art_painting**)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport open_clip\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\nclip_model.eval().to(device)\nfor param in clip_model.parameters():\n    param.requires_grad = False  \n\n# classifier  (512 → 256 → 7)\nclass CLIPMLPClassifier(nn.Module):\n    def __init__(self, clip_model, num_classes):\n        super().__init__()\n        self.clip = clip_model\n        self.mlp = nn.Sequential(\n            nn.Linear(self.clip.visual.output_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        with torch.no_grad():\n            feats = self.clip.encode_image(x)\n        return self.mlp(feats)\n\n\ndef get_loader(domains, root, batch_size=32, shuffle=False):\n    dataset = []\n    for domain in domains:\n        ds = datasets.ImageFolder(os.path.join(root, domain), transform=preprocess)\n        dataset.extend(ds.samples)\n    base_ds = datasets.ImageFolder(os.path.join(root, domains[0]), transform=preprocess)\n    base_ds.samples = dataset\n    loader = DataLoader(base_ds, batch_size=batch_size, shuffle=shuffle)\n    return loader, len(base_ds.classes)\n\n\nPACS_PATH = \"/kaggle/input/pacs-dataset/kfold\"\ntrain_domains = [\"cartoon\", \"photo\", \"sketch\"]\ntest_domain = \"art_painting\"\n\n\ntrain_loader, num_classes = get_loader(train_domains, root=PACS_PATH, shuffle=True)\ntest_loader, _ = get_loader([test_domain], root=PACS_PATH, shuffle=False)\n\n\nmodel = CLIPMLPClassifier(clip_model, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.mlp.parameters(), lr=1e-4)\n\n# Train + evaluate \nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"\\nEpoch {epoch+1} Training Loss: {total_loss:.4f}\")\n\n    \n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = torch.argmax(outputs, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    acc = 100 * correct / total\n    print(f\"Test Accuracy on '{test_domain}': {acc:.2f}%\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T09:04:47.305141Z","iopub.execute_input":"2025-07-17T09:04:47.305615Z","iopub.status.idle":"2025-07-17T09:21:26.756879Z","shell.execute_reply.started":"2025-07-17T09:04:47.305592Z","shell.execute_reply":"2025-07-17T09:21:26.756257Z"}},"outputs":[],"execution_count":null}]}