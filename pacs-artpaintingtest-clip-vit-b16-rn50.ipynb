{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9545143,"sourceType":"datasetVersion","datasetId":5815134}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/mlfoundations/open_clip.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:19:37.358443Z","iopub.execute_input":"2025-07-18T09:19:37.358886Z","iopub.status.idle":"2025-07-18T09:19:44.869052Z","shell.execute_reply.started":"2025-07-18T09:19:37.358860Z","shell.execute_reply":"2025-07-18T09:19:44.867946Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/mlfoundations/open_clip.git\n  Cloning https://github.com/mlfoundations/open_clip.git to /tmp/pip-req-build-9b9g8tbx\n  Running command git clone --filter=blob:none --quiet https://github.com/mlfoundations/open_clip.git /tmp/pip-req-build-9b9g8tbx\n  Resolved https://github.com/mlfoundations/open_clip.git to commit a87f11eaf354000d2736580855ae0d9b76ad2a22\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (0.21.0+cu124)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (2024.11.6)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (6.3.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (0.5.3)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.32.0) (1.0.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch==2.32.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch==2.32.0) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open_clip_torch==2.32.0) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch==2.32.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch==2.32.0) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch==2.32.0) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch==2.32.0) (1.1.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch==2.32.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch==2.32.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch==2.32.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==2.32.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==2.32.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==2.32.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==2.32.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==2.32.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==2.32.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==2.32.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==2.32.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==2.32.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==2.32.0) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open_clip_torch==2.32.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open_clip_torch==2.32.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open_clip_torch==2.32.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open_clip_torch==2.32.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open_clip_torch==2.32.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**no of samples**","metadata":{}},{"cell_type":"code","source":"import os\n\nroot_dir = \"/kaggle/input/pacs-dataset/kfold\"\ndomains = [\"photo\", \"art_painting\", \"cartoon\", \"sketch\"]\n\nfor domain in domains:\n    domain_path = os.path.join(root_dir, domain)\n    count = 0\n    for cls in os.listdir(domain_path):\n        cls_path = os.path.join(domain_path, cls)\n        count += len(os.listdir(cls_path))\n    print(f\"{domain} has {count} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:19:44.870578Z","iopub.execute_input":"2025-07-18T09:19:44.870897Z","iopub.status.idle":"2025-07-18T09:19:44.903041Z","shell.execute_reply.started":"2025-07-18T09:19:44.870871Z","shell.execute_reply":"2025-07-18T09:19:44.902348Z"}},"outputs":[{"name":"stdout","text":"photo has 1670 images\nart_painting has 2048 images\ncartoon has 2344 images\nsketch has 3929 images\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"**Multi-Domain Training with RN50 and ViT-B16 Backbones**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport open_clip\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nPACS_PATH = \"/kaggle/input/pacs-dataset/kfold\"\n\nclass CLIPMLPClassifier(nn.Module):\n    def __init__(self, clip_model, num_classes):\n        super().__init__()\n        self.clip = clip_model\n        self.mlp = nn.Sequential(\n            nn.Linear(self.clip.visual.output_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        with torch.no_grad():\n            feats = self.clip.encode_image(x)\n        return self.mlp(feats)\n\ndef get_loader(domains, root, transform, batch_size=32, shuffle=False):\n    dataset = []\n    for domain in domains:\n        ds = datasets.ImageFolder(os.path.join(root, domain), transform=transform)\n        dataset.extend(ds.samples)\n    base_ds = datasets.ImageFolder(os.path.join(root, domains[0]), transform=transform)\n    base_ds.samples = dataset\n    loader = DataLoader(base_ds, batch_size=batch_size, shuffle=shuffle)\n    return loader, len(base_ds.classes)\n\ntrain_domains = [\"photo\", \"sketch\", \"cartoon\"]\ntest_domain = \"art_painting\"\nresults = {}\n\nbackbones = ['RN50', 'ViT-B-16']\n\nfor backbone in backbones:\n    print(f\"\\n= Training with {backbone} =\")\n\n    clip_model, _, preprocess = open_clip.create_model_and_transforms(backbone, pretrained='openai')\n    clip_model.eval().to(device)\n    for param in clip_model.parameters():\n        param.requires_grad = False\n\n    train_loader, num_classes = get_loader(train_domains, PACS_PATH, preprocess, shuffle=True)\n    test_loader, _ = get_loader([test_domain], PACS_PATH, preprocess, shuffle=False)\n\n    model = CLIPMLPClassifier(clip_model, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.mlp.parameters(), lr=1e-4)\n\n    best_acc = 0\n    wait = 0\n    patience = 10\n\n    for epoch in range(50):\n        model.train()\n        total_loss = 0\n\n        for imgs, labels in tqdm(train_loader, desc=f\"[{backbone}] Epoch {epoch+1} - Training\"):\n            imgs, labels = imgs.to(device), labels.to(device)\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(f\"[{backbone}] Epoch {epoch+1} - Training Loss: {total_loss:.4f}\")\n\n        # Evaluation\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for imgs, labels in test_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                preds = torch.argmax(outputs, dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        acc = 100 * correct / total\n        print(f\"[{backbone}] Test Accuracy on '{test_domain}': {acc:.2f}%\")\n\n        if acc > best_acc:\n            best_acc = acc\n            wait = 0\n            print(f\"[{backbone}] Accuracy improved.\")\n        else:\n            wait += 1\n            print(f\"[{backbone}] No improvement. Wait count: {wait}/{patience}\")\n            if wait >= patience:\n                print(f\"[{backbone}] Early stopping triggered.\")\n                break\n\n    results[backbone] = best_acc\n    print(f\"[{backbone}] Best Accuracy Achieved: {best_acc:.2f}%\")\n\nprint(\"\\n= Summary of Test Accuracies =\")\nfor name, acc in results.items():\n    print(f\"{name:10s}: {acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:19:44.903848Z","iopub.execute_input":"2025-07-18T09:19:44.904111Z","iopub.status.idle":"2025-07-18T10:06:31.127713Z","shell.execute_reply.started":"2025-07-18T09:19:44.904076Z","shell.execute_reply":"2025-07-18T10:06:31.126909Z"}},"outputs":[{"name":"stdout","text":"\n= Training with RN50 =\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 1 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 1 - Training Loss: 437.0409\n[RN50] Test Accuracy on 'art_painting': 54.79%\n[RN50] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 2 - Training: 100%|██████████| 249/249 [00:41<00:00,  6.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 2 - Training Loss: 310.9243\n[RN50] Test Accuracy on 'art_painting': 54.54%\n[RN50] No improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 3 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 3 - Training Loss: 226.6281\n[RN50] Test Accuracy on 'art_painting': 53.52%\n[RN50] No improvement. Wait count: 2/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 4 - Training: 100%|██████████| 249/249 [00:43<00:00,  5.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 4 - Training Loss: 181.5742\n[RN50] Test Accuracy on 'art_painting': 54.64%\n[RN50] No improvement. Wait count: 3/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 5 - Training: 100%|██████████| 249/249 [00:43<00:00,  5.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 5 - Training Loss: 158.8485\n[RN50] Test Accuracy on 'art_painting': 56.10%\n[RN50] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 6 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 6 - Training Loss: 142.7923\n[RN50] Test Accuracy on 'art_painting': 55.03%\n[RN50] No improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 7 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 7 - Training Loss: 132.4231\n[RN50] Test Accuracy on 'art_painting': 57.28%\n[RN50] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 8 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 8 - Training Loss: 125.1948\n[RN50] Test Accuracy on 'art_painting': 54.83%\n[RN50] No improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 9 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 9 - Training Loss: 118.8262\n[RN50] Test Accuracy on 'art_painting': 59.13%\n[RN50] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 10 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 10 - Training Loss: 115.2733\n[RN50] Test Accuracy on 'art_painting': 60.74%\n[RN50] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 11 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 11 - Training Loss: 111.7197\n[RN50] Test Accuracy on 'art_painting': 57.52%\n[RN50] No improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 12 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 12 - Training Loss: 108.2338\n[RN50] Test Accuracy on 'art_painting': 58.69%\n[RN50] No improvement. Wait count: 2/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 13 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 13 - Training Loss: 106.6982\n[RN50] Test Accuracy on 'art_painting': 59.86%\n[RN50] No improvement. Wait count: 3/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 14 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 14 - Training Loss: 102.8364\n[RN50] Test Accuracy on 'art_painting': 59.57%\n[RN50] No improvement. Wait count: 4/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 15 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 15 - Training Loss: 102.0708\n[RN50] Test Accuracy on 'art_painting': 62.11%\n[RN50] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 16 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 16 - Training Loss: 99.8971\n[RN50] Test Accuracy on 'art_painting': 60.55%\n[RN50] No improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 17 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 17 - Training Loss: 98.3502\n[RN50] Test Accuracy on 'art_painting': 61.33%\n[RN50] No improvement. Wait count: 2/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 18 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 18 - Training Loss: 98.8712\n[RN50] Test Accuracy on 'art_painting': 56.84%\n[RN50] No improvement. Wait count: 3/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 19 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 19 - Training Loss: 95.4276\n[RN50] Test Accuracy on 'art_painting': 59.91%\n[RN50] No improvement. Wait count: 4/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 20 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 20 - Training Loss: 96.2718\n[RN50] Test Accuracy on 'art_painting': 60.25%\n[RN50] No improvement. Wait count: 5/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 21 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 21 - Training Loss: 94.6804\n[RN50] Test Accuracy on 'art_painting': 61.04%\n[RN50] No improvement. Wait count: 6/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 22 - Training: 100%|██████████| 249/249 [00:42<00:00,  5.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 22 - Training Loss: 92.3355\n[RN50] Test Accuracy on 'art_painting': 58.45%\n[RN50] No improvement. Wait count: 7/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 23 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 23 - Training Loss: 92.7385\n[RN50] Test Accuracy on 'art_painting': 59.08%\n[RN50] No improvement. Wait count: 8/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 24 - Training: 100%|██████████| 249/249 [00:41<00:00,  6.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 24 - Training Loss: 90.7000\n[RN50] Test Accuracy on 'art_painting': 60.25%\n[RN50] No improvement. Wait count: 9/10\n","output_type":"stream"},{"name":"stderr","text":"[RN50] Epoch 25 - Training: 100%|██████████| 249/249 [00:41<00:00,  5.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RN50] Epoch 25 - Training Loss: 90.1626\n[RN50] Test Accuracy on 'art_painting': 59.23%\n[RN50] No improvement. Wait count: 10/10\n[RN50] Early stopping triggered.\n[RN50] Best Accuracy Achieved: 62.11%\n\n= Training with ViT-B-16 =\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 1 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 1 - Training Loss: 230.5045\n[ViT-B-16] Test Accuracy on 'art_painting': 96.00%\n[ViT-B-16] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 2 - Training: 100%|██████████| 249/249 [01:20<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 2 - Training Loss: 52.2855\n[ViT-B-16] Test Accuracy on 'art_painting': 96.78%\n[ViT-B-16] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 3 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 3 - Training Loss: 34.1994\n[ViT-B-16] Test Accuracy on 'art_painting': 96.83%\n[ViT-B-16] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 4 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 4 - Training Loss: 28.2619\n[ViT-B-16] Test Accuracy on 'art_painting': 97.02%\n[ViT-B-16] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 5 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 5 - Training Loss: 25.0632\n[ViT-B-16] Test Accuracy on 'art_painting': 97.12%\n[ViT-B-16] Accuracy improved.\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 6 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 6 - Training Loss: 22.9768\n[ViT-B-16] Test Accuracy on 'art_painting': 96.92%\n[ViT-B-16] No improvement. Wait count: 1/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 7 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 7 - Training Loss: 21.5907\n[ViT-B-16] Test Accuracy on 'art_painting': 96.92%\n[ViT-B-16] No improvement. Wait count: 2/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 8 - Training: 100%|██████████| 249/249 [01:18<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 8 - Training Loss: 20.3037\n[ViT-B-16] Test Accuracy on 'art_painting': 96.78%\n[ViT-B-16] No improvement. Wait count: 3/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 9 - Training: 100%|██████████| 249/249 [01:18<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 9 - Training Loss: 19.1903\n[ViT-B-16] Test Accuracy on 'art_painting': 96.63%\n[ViT-B-16] No improvement. Wait count: 4/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 10 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 10 - Training Loss: 18.2140\n[ViT-B-16] Test Accuracy on 'art_painting': 96.44%\n[ViT-B-16] No improvement. Wait count: 5/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 11 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 11 - Training Loss: 17.3738\n[ViT-B-16] Test Accuracy on 'art_painting': 96.97%\n[ViT-B-16] No improvement. Wait count: 6/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 12 - Training: 100%|██████████| 249/249 [01:19<00:00,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 12 - Training Loss: 16.8597\n[ViT-B-16] Test Accuracy on 'art_painting': 96.63%\n[ViT-B-16] No improvement. Wait count: 7/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 13 - Training: 100%|██████████| 249/249 [01:20<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 13 - Training Loss: 16.1565\n[ViT-B-16] Test Accuracy on 'art_painting': 96.58%\n[ViT-B-16] No improvement. Wait count: 8/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 14 - Training: 100%|██████████| 249/249 [01:20<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 14 - Training Loss: 15.6309\n[ViT-B-16] Test Accuracy on 'art_painting': 96.48%\n[ViT-B-16] No improvement. Wait count: 9/10\n","output_type":"stream"},{"name":"stderr","text":"[ViT-B-16] Epoch 15 - Training: 100%|██████████| 249/249 [01:20<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"[ViT-B-16] Epoch 15 - Training Loss: 14.8168\n[ViT-B-16] Test Accuracy on 'art_painting': 96.24%\n[ViT-B-16] No improvement. Wait count: 10/10\n[ViT-B-16] Early stopping triggered.\n[ViT-B-16] Best Accuracy Achieved: 97.12%\n\n= Summary of Test Accuracies =\nRN50      : 62.11%\nViT-B-16  : 97.12%\n","output_type":"stream"}],"execution_count":9}]}